{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms ,models\n",
    "import cv2\n",
    "import os \n",
    "import gc\n",
    "import warnings\n",
    "from models.models import ENet,UNet, TempDiscriminator3D\n",
    "from data.datagen_stage3 import DataGenerator\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "\n",
    "#params\n",
    "EPOCHS = 10\n",
    "starting_epoch = 0\n",
    "device = 'cuda'\n",
    "batch_size = 1\n",
    "H,W = (128,128)\n",
    "dataset_len = 8726"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total encoder Parameters 11490240\n",
      "Total Trainable Parameters: 4855335\n",
      "Total TempDiscriminator3D Trainable Parameters: 8369633\n",
      "Total encoder Parameters 927008\n"
     ]
    }
   ],
   "source": [
    "#models\n",
    "encoder = models.video.mc3_18(weights='MC3_18_Weights.KINETICS400_V1')\n",
    "encoder = nn.Sequential(*list(encoder.children())[:-1][:-1]).to(device).eval() #131072 output vector for 1,3,8,256,256\n",
    "trainable_encoder_parameters = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "print(f'Total encoder Parameters {trainable_encoder_parameters}')\n",
    "\n",
    "generator = ENet(in_channels=15, out_channels=3, residual_blocks=64).train().to(device)\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=1e-5,betas=(0.9, 0.9))\n",
    "\n",
    "\n",
    "discriminator = TempDiscriminator3D(d=32)\n",
    "discriminator.to(device).train()\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=1e-5,betas=(0.9, 0.9))\n",
    "\n",
    "mobile = models.mobilenet_v3_small(weights=True)\n",
    "mobile = mobile.features.eval().to(device)\n",
    "trainable_mobile_parameters = sum(p.numel() for p in mobile.parameters() if p.requires_grad)\n",
    "print(f'Total encoder Parameters {trainable_mobile_parameters}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded weights from second training stage generator_3.pth\n"
     ]
    }
   ],
   "source": [
    "#load weights\n",
    "generator_ckpt_dir = './stage3/generator/'\n",
    "discriminator_ckpt_dir = './stage3/discriminator/'\n",
    "ckpts = [x for x in os.listdir(generator_ckpt_dir) if x.endswith('.pth')]\n",
    "if ckpts:\n",
    "    ckpts = sorted(ckpts, key = lambda x : x.split('.')[0].split('_')[1]) #sort\n",
    "    latest = ckpts[-1]\n",
    "    state_dict = torch.load(os.path.join(generator_ckpt_dir,latest))\n",
    "    generator.load_state_dict(state_dict['model'])\n",
    "    starting_epoch = state_dict['epoch'] + 1\n",
    "    g_optimizer.load_state_dict(state_dict['optimizer'])\n",
    "    print('loaded generator weights from previous session')\n",
    "\n",
    "    disc_ckpts = [x for x in os.listdir(discriminator_ckpt_dir) if x.endswith('.pth')]\n",
    "    disc_ckpts = sorted(disc_ckpts, key = lambda x : x.split('.')[0].split('_')[1]) #sort\n",
    "    latest = disc_ckpts[-1]\n",
    "    state_dict = torch.load(os.path.join(discriminator_ckpt_dir,latest))\n",
    "    discriminator.load_state_dict(state_dict['model'])\n",
    "    starting_epoch = state_dict['epoch'] + 1\n",
    "    d_optimizer.load_state_dict(state_dict['optimizer'])\n",
    "    print('loaded weights from previous session')\n",
    "    print(f'starting from epoch {starting_epoch}')\n",
    "else:\n",
    "    ckpt_dir = './stage2/generator/'\n",
    "    ckpts = [x for x in os.listdir(ckpt_dir) if x.endswith('.pth')]\n",
    "    ckpts = sorted(ckpts, key = lambda x : int(x.split('.')[0].split('_')[1])) #sort\n",
    "    latest = ckpts[-1]\n",
    "    state_dict = torch.load(os.path.join(ckpt_dir,latest))\n",
    "    generator.load_state_dict(state_dict['model'])\n",
    "    print(f'loaded weights from second training stage {latest}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss functions\n",
    "def contextual_loss(x, y, h=0.5):\n",
    "    \"\"\"Computes contextual loss between x and y.\n",
    "    Args:\n",
    "      x: features of shape (N, C, H, W).\n",
    "      y: features of shape (N, C, H, W).\n",
    "    Returns:\n",
    "      cx_loss = contextual loss between x and y (Eq (1) in the paper)\n",
    "    \"\"\"\n",
    "    assert x.size() == y.size()\n",
    "    N, C, H, W = x.size()   # e.g., 10 x 512 x 14 x 14. In this case, the number of points is 196 (14x14).\n",
    "    y_mu = y.mean(3).mean(2).mean(0).reshape(1, -1, 1, 1)\n",
    "    x_centered = x - y_mu\n",
    "    y_centered = y - y_mu\n",
    "    x_normalized = x_centered / torch.norm(x_centered, p=2, dim=1, keepdim=True)\n",
    "    y_normalized = y_centered / torch.norm(y_centered, p=2, dim=1, keepdim=True)\n",
    "    # The equation at the bottom of page 6 in the paper\n",
    "    # Vectorized computation of cosine similarity for each pair of x_i and y_j\n",
    "    x_normalized = x_normalized.reshape(N, C, -1)                                # (N, C, H*W)\n",
    "    y_normalized = y_normalized.reshape(N, C, -1)                                # (N, C, H*W)\n",
    "    cosine_sim = torch.bmm(x_normalized.transpose(1, 2), y_normalized)           # (N, H*W, H*W)\n",
    "    d = 1 - cosine_sim                                  # (N, H*W, H*W)  d[n, i, j] means d_ij for n-th data \n",
    "    d_min, _ = torch.min(d, dim=2, keepdim=True)        # (N, H*W, 1)\n",
    "    # Eq (2)\n",
    "    d_tilde = d / (d_min + 1e-5)\n",
    "    # Eq(3)\n",
    "    w = torch.exp((1 - d_tilde) / h)\n",
    "    # Eq(4)\n",
    "    cx_ij = w / torch.sum(w, dim=2, keepdim=True)       # (N, H*W, H*W)\n",
    "    # Eq (1)\n",
    "    cx = torch.mean(torch.max(cx_ij, dim=1)[0], dim=1)  # (N, )\n",
    "    cx_loss = torch.mean(-torch.log(cx + 1e-5))\n",
    "    return cx_loss\n",
    "def perceptual_loss(x, y):\n",
    "    x_norm = x / torch.sqrt(torch.sum(x**2, dim=1, keepdim=True))\n",
    "    y_norm = y / torch.sqrt(torch.sum(y**2, dim=1, keepdim=True))\n",
    "    return torch.sqrt(torch.sum((x_norm - y_norm)**2)) ** 2 / x.numel()\n",
    "def contrastive_motion_loss(stable, unstable, generated):\n",
    "    stable = preprocess(stable)\n",
    "    unstable = preprocess(unstable)\n",
    "    generated = preprocess(generated)\n",
    "    A = encoder(stable).view(batch_size,-1)\n",
    "    A = A / torch.sqrt(torch.sum(A**2, dim=1, keepdim=True))\n",
    "    P = encoder(generated).view(batch_size,-1)\n",
    "    P = P / torch.sqrt(torch.sum(P**2, dim=1, keepdim=True))\n",
    "    N = encoder(unstable).view(batch_size,-1)\n",
    "    N = N / torch.sqrt(torch.sum(N**2, dim=1, keepdim=True))\n",
    "\n",
    "    d1 = torch.mean(torch.sqrt(torch.sum(torch.pow(A - P,2),dim =1)),dim = 0,keepdim=True).to(device) #euclidean distance of vectors\n",
    "    d2 = torch.mean(torch.sqrt(torch.sum(torch.pow(A - N,2),dim =1)),dim = 0,keepdim=True).to(device)#euclidean distance of vectors\n",
    "    return torch.max(d1 - d2 + 1, 0).values\n",
    "\n",
    "binary_cross_entropy = nn.BCELoss()\n",
    "\n",
    "def preprocess(tensor, resize_shape=(128, 171), crop_shape=(112, 112),\n",
    "                        mean=[0.155, 0.161, 0.153], std=[0.228, 0.231, 0.226]):\n",
    "    \"\"\"\n",
    "    Apply transforms to each 3D slice of the 4D tensor along the time dimension.\n",
    "\n",
    "    Args:\n",
    "        tensor (torch.Tensor): 4D tensor of shape [B, C, T, H, W].\n",
    "        resize_shape (tuple): The target size for resizing each 3D slice (H, W).\n",
    "        crop_shape (tuple): The target size for center cropping each 3D slice (H, W).\n",
    "        mean (list): List of mean values for normalization.\n",
    "        std (list): List of standard deviation values for normalization.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Transformed 4D tensor of shape [B, C, T, H, W].\n",
    "    \"\"\"\n",
    "    transforms_3d = transforms.Compose([\n",
    "        transforms.Resize(resize_shape, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.CenterCrop(crop_shape),\n",
    "        transforms.Normalize(mean=mean, std=std),\n",
    "    ])\n",
    "\n",
    "    transformed_slices = []\n",
    "    for t in range(tensor.size(2)):\n",
    "        transformed_slice = transforms_3d(tensor[:, :, t])\n",
    "        transformed_slices.append(transformed_slice.unsqueeze(2))  # Add the time dimension back\n",
    "    return torch.cat(transformed_slices, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = DataGenerator((H,W,3), txt_path='./trainlist_stage3.txt',skip=2)\n",
    "train_ds = iter(data_gen())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('./runs/stage3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0,batch: 0, generator_loss:16.995271682739258 ,per: 0.0008255462162196636, cx: 0.7793654203414917,                  cml: 1.034876823425293,adv: 0.4183591604232788 , discriminator_loss:0.771"
     ]
    }
   ],
   "source": [
    "cv2.namedWindow('window', cv2.WINDOW_NORMAL)\n",
    "g_running_loss = 0\n",
    "d_running_loss = 0\n",
    "for epoch in range(starting_epoch, EPOCHS):\n",
    "    for idx,batch in enumerate(train_ds):\n",
    "        torch.cuda.empty_cache()\n",
    "        input_sequence, unstable_sequence, stable_sequence = batch\n",
    "        generated_sequence = torch.zeros(1,3,8,H,W).float()\n",
    "        g_loss = 0\n",
    "        for k in range(8):\n",
    "            x = input_sequence[k].to(device)\n",
    "            y = stable_sequence[:,:,k,:,:].to(device)\n",
    "            y_hat = generator(x)\n",
    "            generated_sequence[:,:,k,:,:] = y_hat.cpu()\n",
    "            # compute image losses\n",
    "            #get embeddings\n",
    "            feat1 = mobile(y_hat)\n",
    "            feat2 = mobile(y)\n",
    "            percept_loss = perceptual_loss(feat1,feat2)\n",
    "            context_loss = contextual_loss(feat1, feat2)\n",
    "            g_loss += percept_loss + context_loss\n",
    "    \n",
    "        # Update temporal discriminator\n",
    "        d_optimizer.zero_grad()\n",
    "        fake_prediction = discriminator(generated_sequence.detach().to(device))\n",
    "        fake_labels = torch.zeros_like(fake_prediction)\n",
    "        real_prediction = discriminator(stable_sequence.to(device))\n",
    "        real_labels = torch.ones_like(real_prediction)\n",
    "        predictions = torch.cat([fake_prediction, real_prediction], dim=0)\n",
    "        labels = torch.cat([fake_labels, real_labels], dim=0)\n",
    "        d_loss = binary_cross_entropy(predictions, labels)\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        #Update Generator\n",
    "        generated_sequence = generated_sequence.to(device)\n",
    "        unstable_sequence = unstable_sequence.to(device)\n",
    "        stable_sequence = stable_sequence.to(device)\n",
    "        #with torch.no_grad():\n",
    "        score = discriminator(generated_sequence)\n",
    "        labels = torch.ones_like(score).to(device)\n",
    "        generator_adv_loss = binary_cross_entropy(score, labels)\n",
    "        # Contrastive motion loss\n",
    "        encoder.to(device)\n",
    "        cml = contrastive_motion_loss(stable_sequence,\n",
    "                                        unstable_sequence,\n",
    "                                        generated_sequence)\n",
    "        g_loss += generator_adv_loss + 10 * cml \n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        g_running_loss += g_loss.item()\n",
    "        d_running_loss += d_loss.item()\n",
    "        print(f'\\repoch: {epoch},batch: {idx}, generator_loss:{g_running_loss / (idx % 1000 + 1)} ,per: {percept_loss.item()}, cx: {context_loss.item()},\\\n",
    "                  cml: {cml.item()},adv: {generator_adv_loss.item()} , discriminator_loss:{d_loss.item():.3f}',end = '')\n",
    "        del feat1, feat2, percept_loss, context_loss\n",
    "        gc.collect()\n",
    "        \n",
    "        #visualization\n",
    "        means = np.array([0.155,0.161,0.153],dtype = np.float32)\n",
    "        stds = np.array([0.22,0.231,0.226],dtype = np.float32)\n",
    "        img = generated_sequence[:,:,0,:,:].permute(0,2,3,1)[0,...].cpu().detach().numpy()\n",
    "        img *= stds\n",
    "        img += means\n",
    "        img = np.clip(img * 255.0,0,255).astype(np.uint8)\n",
    "        img1 = stable_sequence[:,:,0,:,:].permute(0,2,3,1)[0,...].cpu().numpy()\n",
    "        img1 *= stds\n",
    "        img1 += means\n",
    "        img1 = np.clip(img1 * 255.0,0,255).astype(np.uint8)\n",
    "        concat = cv2.hconcat([img,img1])\n",
    "        cv2.imshow('window',concat)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('9'):\n",
    "            break\n",
    "        if idx % 1000 == 999:\n",
    "            writer.add_scalar('generator_loss',\n",
    "                                g_running_loss / 1000,\n",
    "                                epoch * dataset_len + idx)\n",
    "            writer.add_scalar('discriminator_loss',\n",
    "                                d_running_loss / 1000,\n",
    "                                epoch * dataset_len + idx)\n",
    "            g_running_loss = 0.0\n",
    "            d_running_loss = 0.0\n",
    "            model_path = os.path.join('E:/ModelCkpts/GAN2/stage3/generator/',f'generator_{epoch}.pth')\n",
    "            torch.save({'model':generator.state_dict(),\n",
    "                        'optimizer' : g_optimizer.state_dict(),\n",
    "                        'epoch' : epoch}\n",
    "                    ,model_path)\n",
    "            model_path = os.path.join('E:/ModelCkpts/GAN2/stage3/discriminator/',f'discriminator_{epoch}.pth')\n",
    "            torch.save({'model': discriminator.state_dict(),\n",
    "                        'optimizer' : d_optimizer.state_dict(),\n",
    "                        'epoch' : epoch}\n",
    "                    ,model_path)\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DUTCode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
